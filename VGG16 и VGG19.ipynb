{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи\n",
    "Разберем архитектуру VGG для решения задач распознавания изображений. Применим их для анализа исходных изображений.\n",
    "\n",
    "Используя веса обученной модели и обучим линейный слой поверх существующей модели. Проведем оценку качества предсказания по коэффициенту сходства.\n",
    "\n",
    "Данные:\n",
    "* https://video.ittensive.com/machine-learning/clouds/train.csv.gz (54 Мб)\n",
    "* https://video.ittensive.com/machine-learning/clouds/train_images_small.tar.gz (212 Мб)\n",
    "\n",
    "Соревнование: https://www.kaggle.com/c/understanding_cloud_organization/\n",
    "![](vgg16.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подключение библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Activation\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras import optimizers\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используемые функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "filesDir = \"train_images_small\"\n",
    "image_x = 224 # 525\n",
    "image_y = 224 # 350\n",
    "image_ch = 3 # 3\n",
    "def mask_rate (a, x, y):\n",
    "    b = a//1400 + 0.0\n",
    "    return np.round(x*(b*x//2100) + y*(a%1400)//1400).astype(\"uint32\")\n",
    "\n",
    "def calc_mask (px, x=image_x, y=image_y):\n",
    "    p = np.array([int(n) for n in px.split(' ')]).reshape(-1,2)\n",
    "    mask = np.zeros(x*y, dtype='uint8')\n",
    "    for i, l in p:\n",
    "        mask[mask_rate(i, x, y) - 1:mask_rate(l+i, x, y)] = 1\n",
    "    return mask.reshape(y,x).transpose()\n",
    "\n",
    "def calc_dice (x):\n",
    "    dice = 0\n",
    "    px = x[\"EncodedPixels\"] \n",
    "    if px != px and x[\"target\"] == 0:\n",
    "        dice = 1\n",
    "    elif px == px and x[\"target\"] == 1:\n",
    "        mask = calc_mask(px).flatten()\n",
    "        target = np.ones(image_x*image_y, dtype='uint8')\n",
    "        dice += 2*np.sum(target[mask==1])/(np.sum(target)+np.sum(mask))\n",
    "    return dice\n",
    "\n",
    "def load_y (df):\n",
    "    return np.array(df[\"EncodedPixels\"].notnull().astype(\"int8\")).reshape(len(df), 1)\n",
    "\n",
    "def load_x (df):\n",
    "    x = [[]]*len(df)\n",
    "    for j, file in enumerate(df[\"Image\"]):\n",
    "        img = image.load_img(os.path.join(filesDir, file),\n",
    "                     target_size=(image_y, image_x))\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        x[j] = preprocess_input(img)\n",
    "    return np.array(x).reshape(len(df), image_y, image_x, image_ch)\n",
    "\n",
    "def load_data (df, batch_size):\n",
    "    while True:\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        while batch_start < len(df):\n",
    "            limit = min(batch_end, len(df))\n",
    "            yield (load_x(df[batch_start:limit]),\n",
    "                   load_y(df[batch_start:limit]))\n",
    "            batch_start += batch_size   \n",
    "            batch_end += batch_size\n",
    "\n",
    "def draw_prediction (prediction):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.hist(prediction[0])\n",
    "    ax.set_title(\"Fish\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://video.ittensive.com/machine-learning/clouds/train.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        EncodedPixels        Image Label\n",
      "0   264918 937 266318 937 267718 937 269118 937 27...  0011165.jpg  Fish\n",
      "4   233813 878 235213 878 236613 878 238010 881 23...  002be4f.jpg  Fish\n",
      "8   3510 690 4910 690 6310 690 7710 690 9110 690 1...  0031ae9.jpg  Fish\n",
      "12                                                NaN  0035239.jpg  Fish\n",
      "16  2367966 18 2367985 2 2367993 8 2368002 62 2369...  003994e.jpg  Fish\n"
     ]
    }
   ],
   "source": [
    "data[\"Image\"] = data[\"Image_Label\"].str.split(\"_\").str[0]\n",
    "data[\"Label\"] = data[\"Image_Label\"].str.split(\"_\").str[1]\n",
    "data.drop(labels=[\"Image_Label\"], axis=1, inplace=True)\n",
    "data_fish = data[data[\"Label\"] == \"Fish\"]\n",
    "print (data_fish.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделение данных\n",
    "Разделим всю выборку на 2 части случайным образом: 80% - для обучения модели, 20% - для проверки точности модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           EncodedPixels        Image Label\n",
      "20588  227313 3 227321 9 227332 1 227335 2 227338 1 2...  ed308d4.jpg  Fish\n",
      "10936                                                NaN  7c6474b.jpg  Fish\n",
      "2492                                                 NaN  1bb3c7a.jpg  Fish\n",
      "18864                                                NaN  da6473a.jpg  Fish\n",
      "4316   396958 314 398358 314 399758 314 401158 314 40...  31226bb.jpg  Fish\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(data_fish, test_size=0.2)\n",
    "train = pd.DataFrame(train)\n",
    "test = pd.DataFrame(test)\n",
    "del data\n",
    "print (train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16\n",
    "Используем обученную нейросеть для классификации \"неизвестных\" изображений - облаков\n",
    "\n",
    "Для построения модели потребуются рассчитанные веса: 527 Мб данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = VGG16(weights='imagenet', include_top=False,\n",
    "             input_shape=(image_y, image_x, image_ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25089     \n",
      "=================================================================\n",
      "Total params: 14,739,777\n",
      "Trainable params: 25,089\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layers = Flatten()(vgg16.output)\n",
    "layers = Activation(\"softmax\")(layers)\n",
    "model = Model(inputs=vgg16.input, outputs=Dense(1)(layers))\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "model.compile(optimizer=optimizers.Nadam(lr=0.05),\n",
    "             loss=\"mean_absolute_error\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "221/221 [==============================] - 3237s 15s/step - loss: 0.4575\n",
      "Epoch 2/10\n",
      "221/221 [==============================] - 3191s 14s/step - loss: 0.4690\n",
      "Epoch 3/10\n",
      "221/221 [==============================] - 3502s 16s/step - loss: 0.4628\n",
      "Epoch 4/10\n",
      "221/221 [==============================] - 3380s 15s/step - loss: 0.4655\n",
      "Epoch 5/10\n",
      "221/221 [==============================] - 3769s 17s/step - loss: 0.4630\n",
      "Epoch 6/10\n",
      "221/221 [==============================] - 3340s 15s/step - loss: 0.4639\n",
      "Epoch 7/10\n",
      "221/221 [==============================] - 3390s 15s/step - loss: 0.4596\n",
      "Epoch 8/10\n",
      "221/221 [==============================] - 3610s 16s/step - loss: 0.4631\n",
      "Epoch 9/10\n",
      "221/221 [==============================] - 4163s 19s/step - loss: 0.4645\n",
      "Epoch 10/10\n",
      "221/221 [==============================] - 3658s 17s/step - loss: 0.4627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fde1948>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(load_data(train, batch_size),\n",
    "            epochs=50, steps_per_epoch=len(train)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1110/1110 [==============================] - 1120s 1s/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict_generator(load_data(test, 1),\n",
    "                            steps=len(test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.transpose(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_prediction(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           EncodedPixels  target\n",
      "18428                                                NaN       1\n",
      "13788                                                NaN       1\n",
      "3244                                                 NaN       1\n",
      "8164   1286000 590 1287400 590 1288800 590 1290200 59...       1\n",
      "22044  852912 515 854312 515 855712 515 857112 515 85...       1\n",
      "8112                                                 NaN       1\n",
      "13680  74564 440 75964 440 77364 440 78764 440 80164 ...       1\n",
      "8700   719624 629 721024 629 722424 629 723824 629 72...       1\n",
      "11296  25753 713 27153 713 28553 713 29953 713 31353 ...       1\n",
      "3176   1 506 1401 506 2801 506 4201 506 5601 506 7001...       1\n",
      "21676  650 269 2050 269 3450 269 4850 269 6250 269 76...       1\n",
      "5692   34258 523 35658 523 37058 523 38458 523 39858 ...       1\n",
      "11372  394116 266 394385 5 395516 260 395777 13 39691...       1\n",
      "1616   2262157 238 2263557 238 2264957 238 2266357 23...       1\n"
     ]
    }
   ],
   "source": [
    "test[\"target\"] = np.round(prediction[0]>5).astype(\"int8\")\n",
    "print (test[test[\"target\"]>0][[\"EncodedPixels\",\"target\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расчет точности предсказания\n",
    "Нет облаков - 0.5, MLP - 0.3, CONV - 0.48, AlexNet - 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras, VGG16: 0.496\n"
     ]
    }
   ],
   "source": [
    "dice = test.apply(calc_dice, axis=1, result_type=\"expand\")\n",
    "print (\"Keras, VGG16:\", round(dice.mean(), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
